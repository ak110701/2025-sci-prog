{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "iFmBasp4EQuv"
      },
      "outputs": [],
      "source": [
        "# install & import libraries\n",
        "import google.generativeai as genai\n",
        "import json\n",
        "import time\n",
        "import random\n",
        "import re\n",
        "\n",
        "from google.api_core.exceptions import TooManyRequests\n",
        "from google.colab import userdata\n",
        "\n",
        "# Get your API key from Colab secrets\n",
        "GOOGLE_API_KEY = userdata.get('api_new')\n",
        "\n",
        "genai.configure(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "# Define models\n",
        "thinking_model = genai.GenerativeModel('gemini-2.5-pro')         # big brain model\n",
        "lite_model      = genai.GenerativeModel('gemini-2.5-flash-lite') # model we optimize\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def call_with_retry(func, max_retries=5):\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            return func()\n",
        "        except TooManyRequests as e:\n",
        "            retry_after = getattr(e, \"retry_after\", None)\n",
        "            if retry_after is None:\n",
        "                retry_after = 2 ** attempt + random.random()\n",
        "            print(f\"Quota hit, retrying in {retry_after:.2f} seconds...\")\n",
        "            time.sleep(retry_after)\n",
        "    raise Exception(\"Max retries exceeded\")"
      ],
      "metadata": {
        "id": "9LTvFwEuEUYu"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_with_lite(prompt: str) -> str:\n",
        "    def _call():\n",
        "        response = lite_model.generate_content(prompt)\n",
        "        return getattr(response, \"text\", str(response))\n",
        "    return call_with_retry(_call)\n"
      ],
      "metadata": {
        "id": "IFlwlu9fEUYF"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_feedback_and_optimized_prompt(lite_prompt: str, lite_output: str) -> dict:\n",
        "\n",
        "    optimization_prompt = f\"\"\"\n",
        "You are a prompt optimizer for a small text-generation model (\"lite model\").\n",
        "\n",
        "Your tasks:\n",
        "- Evaluate the original prompt + lite model output.\n",
        "- Identify weaknesses.\n",
        "- Suggest an improved prompt (same goal, but clearer, structured, more helpful).\n",
        "- Explain your feedback.\n",
        "\n",
        "Return ONLY valid JSON:\n",
        "{{\n",
        "  \"optimized_prompt\": \"...\",\n",
        "  \"feedback_text\": \"...\",\n",
        "  \"rationale\": \"...\"\n",
        "}}\n",
        "\n",
        "ORIGINAL_PROMPT:\n",
        "{lite_prompt}\n",
        "\n",
        "LITE_MODEL_OUTPUT:\n",
        "{lite_output}\n",
        "\"\"\"\n",
        "\n",
        "    def _call():\n",
        "        response = thinking_model.generate_content(optimization_prompt)\n",
        "        return getattr(response, \"text\", str(response))\n",
        "\n",
        "    raw_text = call_with_retry(_call).strip()\n",
        "\n",
        "    # extract JSON in case model wrapped it in ```json ... ```\n",
        "    match = re.search(r\"```json\\s*([\\s\\S]*?)\\s*```\", raw_text)\n",
        "    json_str = match.group(1).strip() if match else raw_text\n",
        "\n",
        "    try:\n",
        "        data = json.loads(json_str)\n",
        "    except json.JSONDecodeError:\n",
        "        print(\"Could not parse JSON. Keeping original prompt.\")\n",
        "        print(\"Raw thinking model output:\\n\", raw_text)\n",
        "        return {\n",
        "            \"optimized_prompt\": lite_prompt,\n",
        "            \"feedback_text\": \"Thinking model did not output valid JSON.\",\n",
        "            \"rationale\": \"Prompt unchanged due to JSON parsing failure.\"\n",
        "        }\n",
        "\n",
        "    return {\n",
        "        \"optimized_prompt\": data.get(\"optimized_prompt\", lite_prompt),\n",
        "        \"feedback_text\": data.get(\"feedback_text\", \"\"),\n",
        "        \"rationale\": data.get(\"rationale\", \"\"),\n",
        "    }\n"
      ],
      "metadata": {
        "id": "IpAofF1fE49k"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_optimization_loop(initial_prompt: str, num_turns: int = 5):\n",
        "\n",
        "    current_prompt = initial_prompt\n",
        "    print(\"=== START PROMPT OPTIMIZATION ===\\n\")\n",
        "\n",
        "    for turn in range(1, num_turns + 1):\n",
        "        print(f\"\\n--- TURN {turn} ---\")\n",
        "\n",
        "        print(\"\\nCurrent prompt:\")\n",
        "        print(\"----------------------------------\")\n",
        "        print(current_prompt)\n",
        "\n",
        "        print(\"\\n[1] Calling lite model...\")\n",
        "        lite_output = generate_with_lite(current_prompt)\n",
        "        print(\"----------------------------------\")\n",
        "        print(lite_output)\n",
        "\n",
        "        print(\"\\n[2] Getting improved prompt...\")\n",
        "        feedback = get_feedback_and_optimized_prompt(current_prompt, lite_output)\n",
        "\n",
        "        print(\"\\nFeedback:\")\n",
        "        print(\"----------------------------------\")\n",
        "        print(feedback[\"feedback_text\"])\n",
        "\n",
        "        print(\"\\nRationale:\")\n",
        "        print(\"----------------------------------\")\n",
        "        print(feedback[\"rationale\"])\n",
        "\n",
        "        print(\"\\nNew optimized prompt:\")\n",
        "        print(\"----------------------------------\")\n",
        "        print(feedback[\"optimized_prompt\"])\n",
        "\n",
        "        current_prompt = feedback[\"optimized_prompt\"]\n",
        "\n",
        "    print(\"\\n=== END ===\")\n",
        "    print(\"\\nFinal optimized prompt:\")\n",
        "    print(\"----------------------------------\")\n",
        "    print(current_prompt)\n"
      ],
      "metadata": {
        "id": "UxzFXYz8E62J"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "initial_prompt = \"Napiši uvjerljiv tekst za oglas koji prodaje premium dizajnersku torbu.\"\n",
        "num_iterations = 5\n",
        "\n",
        "run_optimization_loop(initial_prompt, num_iterations)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "fehYgDkVE8Qx",
        "outputId": "06a59c43-6045-448a-baf6-b562ae53cc8f"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== START PROMPT OPTIMIZATION ===\n",
            "\n",
            "\n",
            "--- TURN 1 ---\n",
            "\n",
            "Current prompt:\n",
            "----------------------------------\n",
            "Napiši uvjerljiv tekst za oglas koji prodaje premium dizajnersku torbu.\n",
            "\n",
            "[1] Calling lite model...\n",
            "----------------------------------\n",
            "Naravno, evo prijedloga za uvjerljiv oglas za premium dizajnersku torbu:\n",
            "\n",
            "---\n",
            "\n",
            "**Naslov:** **Više od torbe. Izjava. Vaša jedinstvena priča.**\n",
            "\n",
            "**Glavni tekst:**\n",
            "\n",
            "U svijetu koji žuri, gdje se svakodnevica često pretvara u monotoniju, postoji jedan predmet koji odskače. Predmet koji nosi priču, odražava Vaš stil i naglašava Vašu neusporedivu osobnost. Predstavljamo Vam [Ime Dizajnerske Torbe/Kolekcije], remek-djelo dizajna i umijeća, kreirano za one koji cijene savršenstvo u svakom detalju.\n",
            "\n",
            "Ova torba nije samo modni dodatak; to je produžetak Vas. Izrađena od **najfinije [navedite vrstu kože, npr. talijanske teleće kože, egzotične kože]**, s nepogrešivom pažnjom prema svakom šavu i obliku, ona je sinonim za luksuz koji se osjeti. Mekana na dodir, a istovremeno nevjerojatno izdržljiva, obećava da će biti Vaš vjerni suputnik godinama koje dolaze.\n",
            "\n",
            "Dizajn [Ime Dizajnerske Torbe/Kolekcije] je **bezvremenski, elegantan i nenametljivo sofisticiran**. Njegove čiste linije, savršeni proporcije i luksuzni detalji [navedite specifične detalje, npr. ručno polirane kopče od plemenitog metala, diskretni logotip, jedinstveni patentni zatvarač] govore o luksuzu koji se ne mora dokazivati. Ova torba je stvorena da privuče poglede, ali ne drečavo, već s nepogrešivim osjećajem za stil i klasu.\n",
            "\n",
            "Unutrašnjost torbe jednako je promišljena. Organizirajte svoje najvažnije stvari s lakoćom u **prostranom, ali elegantnom interijeru obloženom [navedite vrstu podstave, npr. svilenom, antilopskom podstavom]**, s pažljivo raspoređenim džepovima koji čuvaju Vaše tajne i potrebe. Bilo da se radi o Vašem laptopu, omiljenoj knjizi ili najnovijem beauty proizvodu, sve će pronaći svoje mjesto u savršenom redu.\n",
            "\n",
            "Posjedovati [Ime Dizajnerske Torbe/Kolekcije] znači odabrati **izvrsnost bez kompromisa**. To je investicija u stil, samopouzdanje i osjećaj pripadnosti klubu onih koji prepoznaju i cijene istinsko umijeće. Nosite je na poslovne sastanke, večernje izlaske ili vikend putovanja – ova torba će podići svaku Vašu odjevnu kombinaciju na novu razinu.\n",
            "\n",
            "**Ne kupujte samo torbu. Uložite u iskustvo. Uložite u sebe.**\n",
            "\n",
            "**Poziv na akciju:**\n",
            "\n",
            "Otkrijte kolekciju [Ime Dizajnerske Torbe/Kolekcije] i pronađite svoj savršeni komad. Posjetite našu ekskluzivnu trgovinu na [Adresa Trgovine] ili istražite cijelu ponudu online na [Web Adresa].\n",
            "\n",
            "**Vrijeme je da svoju priču ispričate s elegancijom.**\n",
            "\n",
            "---\n",
            "\n",
            "**Dodatni savjeti za prodaju:**\n",
            "\n",
            "*   **Vizualna privlačnost:** Ovom tekstu bi definitivno trebale pratiti **vrhunske fotografije** torbe iz različitih kutova, detalji i možda čak i kratki video koji prikazuje teksturu materijala i način na koji torba stoji ili se nosi.\n",
            "*   **Osobnost:** Ako je moguće, uključite ime dizajnera ili inspiraciju iza kolekcije kako biste dodali dubinu i priču.\n",
            "*   **Ograničena dostupnost:** Ako je torba dio ograničene serije, naglasite to kako biste potaknuli hitnost.\n",
            "*   **Iskustvo kupnje:** Spomenite ako postoji ekskluzivno iskustvo pri kupnji (npr. osobni stilist, personalizacija, luksuzno pakiranje).\n",
            "*   **Ciljana publika:** Razmislite o tonu i jeziku koji će najbolje rezonirati s Vašom ciljanom publikom. Za premium proizvode, često je prikladniji sofisticiraniji i elegantniji ton.\n",
            "*   **Društveni dokaz:** Ako je moguće, spomenite ako je proizvod nosila poznata osoba ili ga je preporučio modni magazin.\n",
            "\n",
            "Nadam se da će Vam ovaj prijedlog poslužiti kao dobra osnova!\n",
            "\n",
            "[2] Getting improved prompt...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 254.20ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Quota hit, retrying in 1.58 seconds...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 280.54ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Quota hit, retrying in 2.74 seconds...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 356.35ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Quota hit, retrying in 4.91 seconds...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 254.41ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Quota hit, retrying in 8.17 seconds...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 280.59ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Quota hit, retrying in 16.54 seconds...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "Exception",
          "evalue": "Max retries exceeded",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1637041755.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mnum_iterations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mrun_optimization_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_prompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_iterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-1143656913.py\u001b[0m in \u001b[0;36mrun_optimization_loop\u001b[0;34m(initial_prompt, num_turns)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n[2] Getting improved prompt...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mfeedback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_feedback_and_optimized_prompt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_prompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlite_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nFeedback:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3046569768.py\u001b[0m in \u001b[0;36mget_feedback_and_optimized_prompt\u001b[0;34m(lite_prompt, lite_output)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"text\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mraw_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_with_retry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;31m# extract JSON in case model wrapped it in ```json ... ```\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-74294692.py\u001b[0m in \u001b[0;36mcall_with_retry\u001b[0;34m(func, max_retries)\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Quota hit, retrying in {retry_after:.2f} seconds...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry_after\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Max retries exceeded\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mException\u001b[0m: Max retries exceeded"
          ]
        }
      ]
    }
  ]
}